{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a251c5a3-b812-47aa-a727-8267e14d964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525d7a13-b0b0-4828-bf7f-964e3e554fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_folder = \"C:/Praveen/deribit/book_snapshot_5/\"\n",
    "trade_folder = \"C:/Praveen/deribit/trades/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5e2819-618d-4342-9563-65984c6d3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_book_folder = \"C:/Praveen/Python/AlphaHFT/temp_books/\"\n",
    "modified_trade_folder = \"C:/Praveen/Python/AlphaHFT/temp_trades/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113f5e0f-b22e-4da9-ab5a-977115b9d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"C:/Praveen/Python/AlphaHFT/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd2ce4a-cdff-4b0f-82db-ec71f083ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Praveen/Python/AlphaHFT/data_2024-01-01_BTC-PERPETUAL.csv.gz\n",
      "C:/Praveen/Python/AlphaHFT/data_2024-02-01_BTC-PERPETUAL.csv.gz\n",
      "C:/Praveen/Python/AlphaHFT/data_2024-03-01_BTC-PERPETUAL.csv.gz\n",
      "C:/Praveen/Python/AlphaHFT/data_2024-04-01_BTC-PERPETUAL.csv.gz\n"
     ]
    }
   ],
   "source": [
    "books = [join(book_folder, f) for f in listdir(book_folder) if isfile(join(book_folder, f))]\n",
    "data_dfs = []\n",
    "for book in books:\n",
    "    tokens = os.path.basename(book).split(\"_\")\n",
    "    date_holder = tokens[4]\n",
    "    symbol = tokens[5]\n",
    "    trade = r\"deribit_trades_{0}_{1}\".format(date_holder, symbol)\n",
    "    trade = join(trade_folder, trade)\n",
    "    #book_df = pd.read_csv(book, header=0, index_col='local_timestamp')\n",
    "    #trade_df = pd.read_csv(trade, header=0, index_col='local_timestamp')\n",
    "    data = join(data_folder, r\"data_{0}_{1}\".format(date_holder, symbol))\n",
    "    print(data)\n",
    "    data_df = pd.read_csv(data, header=0, index_col='timestamp')\n",
    "    data_dfs.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49335a3-7da1-4e18-87b6-6a4ff978585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify books\n",
    "def validate_prices(i, curr_ts, data_df, book):\n",
    "    ds = data_df.loc[curr_ts, :]\n",
    "    \n",
    "    if ds.loc[\"bid_price\"] != book.loc[\"bids[0].price\"]:\n",
    "        print(\"bid 0 mismatch: \", i)\n",
    "        print(ds.loc[\"bid_price\"], book.loc[\"bids[0].price\"], bdx[-1])\n",
    "        \n",
    "    if ds.loc[\"ask_price\"] != book.loc[\"asks[0].price\"]:\n",
    "        print(\"ask 0 mismatch: \", i)\n",
    "        print(ds.loc[\"ask_price\"], book.loc[\"asks[0].price\"], bdx[-1])\n",
    "\n",
    "    bid_1 = ds.loc[\"bid_price\"] * (1 + ds.loc[\"bid_ticks_1\"] / 1000)\n",
    "    if not np.allclose(bid_1, book.loc[\"bids[1].price\"]):\n",
    "        print(\"bid 1 mismatch: \", i)\n",
    "\n",
    "    ask_1 = ds.loc[\"ask_price\"] * (1 + ds.loc[\"ask_ticks_1\"] / 1000)\n",
    "    if not np.allclose(ask_1, book.loc[\"asks[1].price\"]):\n",
    "        print(\"ask 1 mismatch: \", i)\n",
    "\n",
    "    bid_2 = ds.loc[\"bid_price\"] * ds.loc[\"bid_ticks_2\"] / 1000 + bid_1\n",
    "    if not np.allclose(bid_2, book.loc[\"bids[2].price\"]):\n",
    "        print(\"bid 2 mismatch: \", i)\n",
    "\n",
    "    ask_2 = ds.loc[\"ask_price\"] * ds.loc[\"ask_ticks_2\"] / 1000 + ask_1\n",
    "    if not np.allclose(ask_2, book.loc[\"asks[2].price\"]):\n",
    "        print(\"ask 2 mismatch: \", i)\n",
    "\n",
    "    bid_3 = ds.loc[\"bid_price\"] * ds.loc[\"bid_ticks_3\"] / 1000 + bid_2\n",
    "    if not np.allclose(bid_3, book.loc[\"bids[3].price\"]):\n",
    "        print(\"bid 3 mismatch: \", i)\n",
    "\n",
    "    ask_3 = ds.loc[\"ask_price\"] * ds.loc[\"ask_ticks_3\"] / 1000 + ask_2\n",
    "    if not np.allclose(ask_3, book.loc[\"asks[3].price\"]):\n",
    "        print(\"ask 3 mismatch: \", i)\n",
    "\n",
    "    bid_4 = ds.loc[\"bid_price\"] * ds.loc[\"bid_ticks_4\"] / 1000 + bid_3\n",
    "    if not np.allclose(bid_4, book.loc[\"bids[4].price\"]):\n",
    "        print(\"bid 4 mismatch: \", i)\n",
    "\n",
    "    ask_4 = ds.loc[\"ask_price\"] * ds.loc[\"ask_ticks_4\"] / 1000 + ask_3\n",
    "    if not np.allclose(ask_4, book.loc[\"asks[4].price\"]):\n",
    "        print(\"ask 4 mismatch: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e447143b-3a76-40f0-8c1e-280db5948cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts = None\n",
    "#book_df.index = pd.to_datetime(book_df.index, unit='us')\n",
    "#book_df = book_df.resample(\"1s\").last().ffill()\n",
    "\n",
    "#for i in range(0, data_df.shape[0]):\n",
    "#    curr_ts = data_df.index[i]\n",
    "#    bdx = book_df.index[book_df.index == curr_ts]\n",
    "\n",
    "#    book = book_df.loc[bdx[-1], :]\n",
    "#    validate_prices(i, curr_ts, data_df, book)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c24bbe-11f0-4e78-903c-43a3c3810493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86398, 46)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9a700d-f7f3-4d49-93eb-ab1889af75c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid_incr</th>\n",
       "      <th>ask_incr</th>\n",
       "      <th>bid_ticks_1</th>\n",
       "      <th>bid_ticks_2</th>\n",
       "      <th>bid_ticks_3</th>\n",
       "      <th>bid_ticks_4</th>\n",
       "      <th>ask_ticks_1</th>\n",
       "      <th>ask_ticks_2</th>\n",
       "      <th>ask_ticks_3</th>\n",
       "      <th>ask_ticks_4</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_vwap</th>\n",
       "      <th>ask_vwap</th>\n",
       "      <th>of1</th>\n",
       "      <th>of2</th>\n",
       "      <th>of3</th>\n",
       "      <th>of4</th>\n",
       "      <th>long_size_diff</th>\n",
       "      <th>short_size_diff</th>\n",
       "      <th>long_price_diff</th>\n",
       "      <th>short_price_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-04-01 00:00:02</th>\n",
       "      <td>-0.196209</td>\n",
       "      <td>-0.350322</td>\n",
       "      <td>-0.049052</td>\n",
       "      <td>-0.077082</td>\n",
       "      <td>-0.091097</td>\n",
       "      <td>-0.007007</td>\n",
       "      <td>0.049045</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.056051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462750</td>\n",
       "      <td>0.407885</td>\n",
       "      <td>-8.508324e-08</td>\n",
       "      <td>-2.919157e-06</td>\n",
       "      <td>-7.147271e-08</td>\n",
       "      <td>-1.983495e-09</td>\n",
       "      <td>-0.999818</td>\n",
       "      <td>-0.859162</td>\n",
       "      <td>-71391.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01 00:00:03</th>\n",
       "      <td>-0.049055</td>\n",
       "      <td>-0.189209</td>\n",
       "      <td>-0.077086</td>\n",
       "      <td>-0.042047</td>\n",
       "      <td>-0.098109</td>\n",
       "      <td>-0.049055</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.042047</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.028031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558403</td>\n",
       "      <td>0.124533</td>\n",
       "      <td>-5.838243e-09</td>\n",
       "      <td>-4.705398e-07</td>\n",
       "      <td>-2.806406e-06</td>\n",
       "      <td>-8.356252e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.969572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bid_incr  ask_incr  bid_ticks_1  bid_ticks_2  \\\n",
       "timestamp                                                           \n",
       "2024-04-01 00:00:02 -0.196209 -0.350322    -0.049052    -0.077082   \n",
       "2024-04-01 00:00:03 -0.049055 -0.189209    -0.077086    -0.042047   \n",
       "\n",
       "                     bid_ticks_3  bid_ticks_4  ask_ticks_1  ask_ticks_2  \\\n",
       "timestamp                                                                 \n",
       "2024-04-01 00:00:02    -0.091097    -0.007007     0.049045     0.007006   \n",
       "2024-04-01 00:00:03    -0.098109    -0.049055     0.021023     0.042047   \n",
       "\n",
       "                     ask_ticks_3  ask_ticks_4  ...  bid_vwap  ask_vwap  \\\n",
       "timestamp                                      ...                       \n",
       "2024-04-01 00:00:02     0.007006     0.056051  ...  0.462750  0.407885   \n",
       "2024-04-01 00:00:03     0.021023     0.028031  ...  0.558403  0.124533   \n",
       "\n",
       "                              of1           of2           of3           of4  \\\n",
       "timestamp                                                                     \n",
       "2024-04-01 00:00:02 -8.508324e-08 -2.919157e-06 -7.147271e-08 -1.983495e-09   \n",
       "2024-04-01 00:00:03 -5.838243e-09 -4.705398e-07 -2.806406e-06 -8.356252e-09   \n",
       "\n",
       "                     long_size_diff  short_size_diff  long_price_diff  \\\n",
       "timestamp                                                               \n",
       "2024-04-01 00:00:02       -0.999818        -0.859162         -71391.5   \n",
       "2024-04-01 00:00:03        0.000000        -0.969572              0.0   \n",
       "\n",
       "                     short_price_diff  \n",
       "timestamp                              \n",
       "2024-04-01 00:00:02          0.000000  \n",
       "2024-04-01 00:00:03         -0.000161  \n",
       "\n",
       "[2 rows x 46 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27827fa7-c2f8-4070-a5c3-3d241136e882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imbalance</th>\n",
       "      <th>of1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-04-01 00:00:02</th>\n",
       "      <td>-0.513299</td>\n",
       "      <td>-8.508324e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01 00:00:03</th>\n",
       "      <td>-0.209600</td>\n",
       "      <td>-5.838243e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01 00:00:04</th>\n",
       "      <td>-0.354921</td>\n",
       "      <td>-6.307055e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01 00:00:05</th>\n",
       "      <td>-0.220819</td>\n",
       "      <td>1.363746e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01 00:00:06</th>\n",
       "      <td>-0.931791</td>\n",
       "      <td>-6.600507e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     imbalance           of1\n",
       "timestamp                                   \n",
       "2024-04-01 00:00:02  -0.513299 -8.508324e-08\n",
       "2024-04-01 00:00:03  -0.209600 -5.838243e-09\n",
       "2024-04-01 00:00:04  -0.354921 -6.307055e-08\n",
       "2024-04-01 00:00:05  -0.220819  1.363746e-07\n",
       "2024-04-01 00:00:06  -0.931791 -6.600507e-08"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[['imbalance', 'of1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "179d6d05-a05d-4aeb-ad6e-f16e73063da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class HftDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tick_size, look_back=120):\n",
    "        self.data = df.drop([\"bid_price\", \"ask_price\"], axis=1)\n",
    "        self.book = df[[\"bid_price\", \"ask_price\"]]\n",
    "        self.tick_size = tick_size\n",
    "        self.look_back = look_back\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - 11\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data.iloc[index:index+self.look_back, :].values\n",
    "        idx = self.data.index[index:index+self.look_back]\n",
    "        next_idx = self.book.index[index+1:index+self.look_back+1]\n",
    "        curr_bid = self.book.loc[idx, \"bid_price\"].values\n",
    "        curr_ask = self.book.loc[idx, \"ask_price\"].values\n",
    "        \n",
    "        next_bid = self.book.loc[next_idx, \"bid_price\"].values\n",
    "        next_ask = self.book.loc[next_idx, \"ask_price\"].values\n",
    "\n",
    "        Y = np.zeros(self.look_back)\n",
    "\n",
    "        for i in range(next_bid.shape[0]):\n",
    "            y = max(next_bid[i] - curr_bid[i], 0) * max(next_ask[i] - curr_ask[i], 0) + \\\n",
    "                max(curr_bid[i] - next_bid[i], 0) * max(curr_ask[i] - next_ask[i], 0) \n",
    "            y = np.sqrt(y)\n",
    "            Y[i] = y\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d7ef15-94e8-40aa-b5ca-875cb0a55c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 torch.Size([64, 120, 44])\n"
     ]
    }
   ],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 0,\n",
    "          'drop_last': True}\n",
    "\n",
    "train_ds = HftDataset(data_dfs[0], 0.5)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, **params)\n",
    "valid_ds = HftDataset(data_dfs[1], 0.5)\n",
    "val_loader = torch.utils.data.DataLoader(valid_ds, **params)\n",
    "X_batch_inputs = 0\n",
    "for X_batch, y_batch in train_loader:\n",
    "    X_batch_inputs = X_batch.shape[2]\n",
    "    print(X_batch_inputs, X_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d22e65f-7403-4976-b8f9-ffce419dadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        \"\"\"The __init__ method that initiates a GRU instance.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The number of nodes in the input layer\n",
    "            hidden_dim (int): The number of nodes in each layer\n",
    "            layer_dim (int): The number of layers in the network\n",
    "            output_dim (int): The number of nodes in the output layer\n",
    "            dropout_prob (float): The probability of nodes being dropped out\n",
    "\n",
    "        \"\"\"\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim \n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward method takes input tensor x and does forward propagation\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of the shape (batch size, sequence length, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of the shape (batch size, output_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e1cc5b3-1799-4b9a-a48e-bffc9e0b3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Optimization:\n",
    "    \"\"\"Optimization is a helper class that allows training, validation, prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (RNNModel, LSTMModel, GRUModel): Model class created for the type of RNN\n",
    "            loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses\n",
    "            optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def train_step(self, x, y):\n",
    "        \"\"\"The method train_step completes one step of training.\n",
    "\n",
    "        Given the features (x) and the target values (y) tensors, the method completes\n",
    "        one step of the training. First, it activates the train mode to enable back prop.\n",
    "        After generating predicted values (yhat) by doing forward propagation, it calculates\n",
    "        the losses by using the loss function. Then, it computes the gradients by doing\n",
    "        back propagation and updates the weights by calling step() function.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor for features to train one step\n",
    "            y (torch.Tensor): Tensor for target values to calculate losses\n",
    "\n",
    "        \"\"\"\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "        \"\"\"The method train performs the model training\n",
    "\n",
    "        The method takes DataLoaders for training and validation datasets, batch size for\n",
    "        mini-batch training, number of epochs to train, and number of features as inputs.\n",
    "        Then, it carries out the training by iteratively calling the method train_step for\n",
    "        n_epochs times. If early stopping is enabled, then it  checks the stopping condition\n",
    "        to decide whether the training needs to halt before n_epochs steps. Finally, it saves\n",
    "        the model in a designated file path.\n",
    "\n",
    "        Args:\n",
    "            train_loader (torch.utils.data.DataLoader): DataLoader that stores training data\n",
    "            val_loader (torch.utils.data.DataLoader): DataLoader that stores validation data\n",
    "            batch_size (int): Batch size for mini-batch training\n",
    "            n_epochs (int): Number of epochs, i.e., train steps, to train\n",
    "            n_features (int): Number of feature columns\n",
    "\n",
    "        \"\"\"\n",
    "        model_path = f'{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            print(\"epoch:\", epoch)\n",
    "            for x_batch, y_batch in tqdm(train_loader):\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch[:, -1])\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(y_val[:, -1], yhat[:, 0]).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "        \"\"\"The method evaluate performs the model evaluation\n",
    "\n",
    "        The method takes DataLoaders for the test dataset, batch size for mini-batch testing,\n",
    "        and number of features as inputs. Similar to the model validation, it iteratively\n",
    "        predicts the target values and calculates losses. Then, it returns two lists that\n",
    "        hold the predictions and the actual values.\n",
    "\n",
    "        Note:\n",
    "            This method assumes that the prediction from the previous step is available at\n",
    "            the time of the prediction, and only does one-step prediction into the future.\n",
    "\n",
    "        Args:\n",
    "            test_loader (torch.utils.data.DataLoader): DataLoader that stores test data\n",
    "            batch_size (int): Batch size for mini-batch training\n",
    "            n_features (int): Number of feature columns\n",
    "\n",
    "        Returns:\n",
    "            list[float]: The values predicted by the model\n",
    "            list[float]: The actual values in the test set.\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test)\n",
    "                yhat=yhat.cpu().data.numpy()\n",
    "                predictions.append(yhat)\n",
    "                y_test=y_test.cpu().data.numpy()\n",
    "                values.append(y_test)\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        \"\"\"The method plots the calculated loss values for training and validation\n",
    "        \"\"\"\n",
    "        plt.style.use('ggplot')\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2d5ea46-e41e-42aa-9787-6e817f8ef41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def General_Settings(input_dim):\n",
    "        output_dim = 1\n",
    "        hidden_dim = 64\n",
    "        layer_dim = 3\n",
    "        batch_size = 64\n",
    "        dropout = 0.2\n",
    "        n_epochs = 20\n",
    "        learning_rate = 1e-3\n",
    "        weight_decay = 1e-6\n",
    "\n",
    "        model_params = {'input_dim': input_dim,\n",
    "                        'hidden_dim' : hidden_dim,\n",
    "                        'layer_dim' : layer_dim,\n",
    "                        'output_dim' : output_dim,\n",
    "                        'dropout_prob' : dropout}\n",
    "        \n",
    "        model = GRUModel(**model_params)\n",
    "        loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        opt = Optimization(model=model.to(device), loss_fn=loss_fn, optimizer=optimizer)\n",
    "        opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "        opt.plot_losses()\n",
    "\n",
    "        predictions, values = opt.evaluate(\n",
    "            test_loader_one,\n",
    "            batch_size=1,\n",
    "            n_features=input_dim\n",
    "        )\n",
    "        return predictions, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e8ec034-010c-4b0c-b5cf-29e5b38cbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def format_predictions(predictions, values):\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds})\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    result_metrics = {'mae' : mean_absolute_error(df.value, df.prediction),\n",
    "                      'rmse' : mean_squared_error(df.value, df.prediction) ** 0.5,\n",
    "                      'r2' : r2_score(df.value, df.prediction)}\n",
    "    \n",
    "    print(\"Mean Absolute Error:       \", result_metrics[\"mae\"])\n",
    "    print(\"Root Mean Squared Error:   \", result_metrics[\"rmse\"])\n",
    "    print(\"R^2 Score:                 \", result_metrics[\"r2\"])\n",
    "    return result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e604d8ad-4e0e-4298-83c1-c094315cd1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 1348/1349 [23:45<00:01,  1.06s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [120, 44] at entry 0 and [119, 44] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_default_dtype(torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m----> 3\u001b[0m predictions, values\u001b[38;5;241m=\u001b[39mGeneral_Settings(X_batch_inputs)\n\u001b[0;32m      4\u001b[0m df_result\u001b[38;5;241m=\u001b[39mformat_predictions(predictions, values)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(calculate_metrics(df_result))\n",
      "Cell \u001b[1;32mIn[38], line 23\u001b[0m, in \u001b[0;36mGeneral_Settings\u001b[1;34m(input_dim)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m     22\u001b[0m opt \u001b[38;5;241m=\u001b[39m Optimization(model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device), loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m---> 23\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain(train_loader, val_loader, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, n_epochs\u001b[38;5;241m=\u001b[39mn_epochs, n_features\u001b[38;5;241m=\u001b[39minput_dim)\n\u001b[0;32m     24\u001b[0m opt\u001b[38;5;241m.\u001b[39mplot_losses()\n\u001b[0;32m     26\u001b[0m predictions, values \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m     27\u001b[0m     test_loader_one,\n\u001b[0;32m     28\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     29\u001b[0m     n_features\u001b[38;5;241m=\u001b[39minput_dim\n\u001b[0;32m     30\u001b[0m )\n",
      "Cell \u001b[1;32mIn[37], line 77\u001b[0m, in \u001b[0;36mOptimization.train\u001b[1;34m(self, train_loader, val_loader, batch_size, n_epochs, n_features)\u001b[0m\n\u001b[0;32m     75\u001b[0m batch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     78\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mview([batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_features])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     79\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:222\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [120, 44] at entry 0 and [119, 44] at entry 1"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_dtype(torch.float64)\n",
    "predictions, values=General_Settings(X_batch_inputs)\n",
    "df_result=format_predictions(predictions, values)\n",
    "print(calculate_metrics(df_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab6810-63cf-4e40-9c74-0db3d218ab39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
