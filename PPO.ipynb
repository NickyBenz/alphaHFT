{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19357d86-73e6-41a5-9009-9d02827ef9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490f1a11-8c9a-4a34-ab33-58225b1afdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulator.Exchange import Exchange\n",
    "from Simulator.Strategy import Strategy\n",
    "from Simulator.Order import Order\n",
    "from Simulator.OrderState import OrderState\n",
    "from TradeEnv.TradeGym import TradeEnv\n",
    "from Simulator.InverseInstrument import InverseInstrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0c482f-1453-47b7-97ea-b9c269b7a685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 count:  86392 index:  75009\n",
      "{'balance': 0.002, 'trade_count': 131, 'trading_pnl_pct': -0.0, 'inventory_pnl_pct': -0.25, 'leverage': 3.31, 'reward': -3.56, 'steps': 1200}\n",
      "{'balance': 0.002, 'trade_count': 247, 'trading_pnl_pct': -0.26, 'inventory_pnl_pct': -0.26, 'leverage': 3.32, 'reward': -3.58, 'steps': 2400}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m             model\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[1;32m---> 25\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlearn(length, progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m         clear_output(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#vec_env = model.get_env()\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=20, warn=False)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#print(mean_reward)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:454\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfRecurrentPPO,\n\u001b[0;32m    447\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    453\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfRecurrentPPO:\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    455\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    456\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    457\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    458\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    459\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    460\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer, n_rollout_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps)\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:257\u001b[0m, in \u001b[0;36mRecurrentPPO.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mon_step():\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:134\u001b[0m, in \u001b[0;36mBaseCallback.update_locals\u001b[1;34m(self, locals_)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_locals\u001b[39m(\u001b[38;5;28mself\u001b[39m, locals_: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Update the references to the local variables.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    :param locals_: the local variables during rollout collection\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocals\u001b[38;5;241m.\u001b[39mupdate(locals_)\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_child_locals(locals_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"*.csv.gz\")\n",
    "\n",
    "model = None\n",
    "\n",
    "for file in files[:3]:\n",
    "    for j in range(0, 100):\n",
    "        df = pd.read_csv(file, header=0, index_col='timestamp', parse_dates=['timestamp'])\n",
    "        row_count = df.shape[0]\n",
    "        index = np.random.randint(low=0, high=row_count-7200)\n",
    "        print(\"iteration: \", j, \"count: \", row_count, \"index: \", index)\n",
    "        length = row_count - index + 1\n",
    "        instrument = InverseInstrument(\"BTC-PERPETUAL\", 0.5, 10, 0, 0.0005)\n",
    "        exchange = Exchange(df.iloc[index:, :])\n",
    "        strategy = Strategy(instrument, exchange, 0.002, 0.0002)\n",
    "        env = TradeEnv(strategy, \"human\")\n",
    "    \n",
    "        if model is None:\n",
    "            model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=0, gamma=.999, n_steps=120)\n",
    "        else:\n",
    "            model.set_env(env)\n",
    "        \n",
    "        model = model.learn(length, progress_bar=False)\n",
    "        clear_output(True)\n",
    "\n",
    "#vec_env = model.get_env()\n",
    "#mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=20, warn=False)\n",
    "#print(mean_reward)\n",
    "\n",
    "model.save(\"ppo_recurrent\")\n",
    "del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fd9e9-8c69-4652-a59f-2c7cf74730ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentPPO.load(\"ppo_recurrent\")\n",
    "\n",
    "df = pd.read_csv(files[3], header=0, index_col='timestamp', parse_dates=['timestamp'])\n",
    "exchange = Exchange(df)\n",
    "strategy = Strategy(instrument, exchange, 0.02, 0.0002)\n",
    "env = TradeEnv(strategy, \"human\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "# cell and hidden state of the LSTM\n",
    "lstm_states = None\n",
    "\n",
    "episode_start = 1\n",
    "done = False\n",
    "truncated = False\n",
    "while not done and not truncated:\n",
    "    action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_start, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697062f-49fa-4c52-b056-c1de6879daf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
